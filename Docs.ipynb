{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AutoHyper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoHyper is designed to facilitate hyperparameter optimization (HPO) for supervised learning models on tabular data.\n",
    "It serves as a lightweight, modular, and fully customizable package, giving you fine-grained control over the entire tuning and validation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is AutoHyper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoHyper is designed to:\n",
    "\n",
    "- Provide a clear and consistent interface for different HPO strategies: at the moment grid search, random search, and evolutionary algorithms.\n",
    "\n",
    "- Leverage nested cross-validation to deliver robust and unbiased estimates of out-of-sample model performance.\n",
    "\n",
    "- Incorporate a custom selection mechanism that combines performance and robustness using a weighted scoring function, ensuring the best configurations are both accurate and consistently effective across multiple resampling iterations.\n",
    "\n",
    "- Return structured outputs, ideal for quantitative comparison and visual inspection of configurations.\n",
    "\n",
    "- Offer detailed logging and configuration ranking based on a composite score of performance and frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Components of Hyperparameter Optimization in AutoHyper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical hyperparameter optimization (HPO) problem consists of **five essential components** . AutoHyper is designed to give users full control and flexibility over each of these:\n",
    "\n",
    "1. **`Learner`**\n",
    "\n",
    "The learner is the machine learning model to be tuned. In AutoHyper, any supervised model following the **scikit-learn API** is supported. This includes both classifiers and regressors such as RandomForestClassifier, XGBRegressor, LogisticRegression, and even custom models wrapped using **SciKeras** (for Keras) or **Skorch** (for PyTorch). The only requirement is that the model must implement `fit(X, y)`, `predict(X)`, and `set_params(**kwargs)`.\n",
    "\n",
    "2. **`Hyperparameter Space`**\n",
    "\n",
    "The search space defines the set of hyperparameters to explore. AutoHyper allows users to specify this space as a dictionary of parameter names and candidate values. The search space is dynamically parsed based on the selected optimization strategy. When needed particularly in random search or evolutionary algorithms, AutoHyper can **automatically infer and apply the most appropriate value scale** for each hyperparameter, such as linear sampling for float and integer parameters, and categorical sampling for discrete options. This enables more efficient exploration, especially when dealing with large or non-uniform hyperparameter domains.\n",
    "\n",
    "3. **`Dataset`**\n",
    "\n",
    "AutoHyper is specifically tailored for **tabular datasets**, where `X` is a `pandas.DataFrame` and `y` is a `pandas.Series` or `numpy.ndarray`. The dataset is passed directly to the HPO class and internally split according to the specified resampling strategy. The package assumes clean, preprocessed data, leaving feature engineering and preprocessing under the user's control for maximal transparency and modularity.\n",
    "\n",
    "4. **`Resampling Strategy`** \n",
    "\n",
    "\n",
    "To avoid overfitting and ensure unbiased evaluation, AutoHyper leverages **nested cross-validation**. The outer loop estimates generalization performance, while the inner loop performs hyperparameter tuning. Users can configure the number of outer and inner folds (`n_outer_folds`, `n_inner_folds`), making the resampling strategy fully customizable. This separation guarantees a rigorous assessment of how the chosen hyperparameters would perform on truly unseen data.\n",
    "\n",
    "5. **`Performance Measure`**\n",
    "\n",
    "Evaluation is handled using common metrics like accuracy, f1, precision, recall for classification and r2, neg_mean_squared_error, etc., for regression. The user specifies the metric via the scoring parameter. Internally, AutoHyper calculates average performance for each configuration across all outer folds and applies a **custom weighted scoring function** that balances average performance with robustness (measured as frequency of selection), ensuring the final recommendation is both strong and stable.\n",
    "\n",
    "6. **`Optimization Strategy`**\n",
    "AutoHyper supports multiple optimization strategies, including:\n",
    "- **Grid Search**: Exhaustively explores all combinations of hyperparameters.\n",
    "- **Random Search**: Samples a fixed number of random configurations from the hyperparameter space.\n",
    "- **Evolutionary Algorithms**: Uses genetic algorithms to evolve hyperparameter configurations over generations, balancing exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimization Strategies - How To Use AutoHyper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install AutoHyper simply run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/Christian-Braga/AutoHyper.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Importing Autohyper and setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the HPO class from the autohyper module\n",
    "from autohyper import HPO\n",
    "\n",
    "# Import necessary libraries for your project\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing Dataset and convert it into a pandas DataFrame\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "X = pd.DataFrame(\n",
    "    X,\n",
    "    columns=[\n",
    "        \"MedInc\",\n",
    "        \"HouseAge\",\n",
    "        \"AveRooms\",\n",
    "        \"AveBedrms\",\n",
    "        \"Population\",\n",
    "        \"AveOccup\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "    ],\n",
    ")\n",
    "y = pd.Series(y, name=\"target\")\n",
    "\n",
    "data_features = X\n",
    "data_target = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model, Hyperparameter space and Task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model you want to optimize\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Define the Hyperparameter Space\n",
    "hp_values = {\n",
    "    \"max_depth\": [3, 10, 40],\n",
    "    \"min_samples_split\": [2, 10, 30],\n",
    "    \"min_samples_leaf\": [1, 4, 15],\n",
    "    \"min_weight_fraction_leaf\": [0.0, 0.05, 0.2],\n",
    "}\n",
    "\n",
    "# Specify the task type\n",
    "task = \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create an istance of the HPO class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Required:**\n",
    "\n",
    "- **`model`**:\n",
    "\n",
    "A scikit-learn compatible estimator implementing fit and predict.\n",
    "Used as the base model for hyperparameter tuning.\n",
    "\n",
    "- **`data_features`** (pd.DataFrame):\n",
    "\n",
    "The input features (X) on which training and evaluation are performed.\n",
    "\n",
    "- **`data_target`** (pd.Series or np.ndarray):\n",
    "\n",
    "The target variable (y) to predict.\n",
    "\n",
    "- **`hp_values`** (dict):\n",
    "\n",
    "Dictionary defining the hyperparameter search space.\n",
    "Keys are parameter names, values are lists of candidate values (for grid, random, EA).\n",
    "\n",
    "- **`task`** (str):\n",
    "\n",
    "Type of supervised learning task. Supported: \"classification\" or \"regression\".\n",
    "Determines the scoring metric used in cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-28 15:59:35] [\u001b[32mINFO\u001b[0m] Initialized HPO class\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_hpo = HPO(\n",
    "    model=model,\n",
    "    data_features=data_features,\n",
    "    data_target=data_target,\n",
    "    hp_values=hp_values,\n",
    "    task=task,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Perform Hyperparameter Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the tuning process and obtain the best hyperparameter configuration, call the `hp_tuning` method on the HPO instance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`hp_tuning`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hp_tuning method is the core interface to perform hyperparameter optimization using nested cross-validation. It supports multiple search strategies (**grid_search**, **random_search**, **evolutionary_algorithm**) and returns a structured summary of results, suitable for downstream analysis and visualization.\n",
    "\n",
    "##### **Parameters**\n",
    "\n",
    "| Name                          | Type                    | Description                                                                                                        |\n",
    "| ----------------------------- | ----------------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
    "| `hpo_method`                  | `str`                   | Hyperparameter optimization method to use. Options: `\"grid_search\"`, `\"random_search\"`, `\"evolutionary_algorithm\"` |\n",
    "| `outer_k`                     | `int`                   | Number of folds in the **outer cross-validation** loop (used to evaluate generalization performance)               |\n",
    "| `inner_k`                     | `int`                   | Number of folds in the **inner cross-validation** loop (used for model selection)                                  |\n",
    "| `n_trials`                    | `Optional[int]`         | (Only for `random_search`) Number of random configurations to sample                                               |\n",
    "| `parents_selection_mechanism` | `Optional[str]`         | (Only for `evolutionary_algorithm`) Strategy for selecting parent individuals (e.g. `\"tournament_selection\"`)  |\n",
    "| `parents_selection_ratio`     | `Optional[float]`       | (Only for `evolutionary_algorithm`) Fraction of population selected as parents                                     |\n",
    "| `max_generations`             | `Optional[int]`         | (Only for `evolutionary_algorithm`) Number of generations to evolve                                                |\n",
    "| `n_new_configs`               | `Optional[int]`         | (Only for `evolutionary_algorithm`) Number of new configurations generated per generation                          |\n",
    "| `shuffle`                     | `bool` (default `True`) | Whether to shuffle data before splitting in CV                                                                     |\n",
    "\n",
    "##### **Returns**\n",
    "```python\n",
    "Dict[str, Any]\n",
    "```\n",
    "A structured dictionary with the following keys:\n",
    "\n",
    "- **`best_config`**: Best configuration by weighted score (70% performance, 30% frequency)\n",
    "\n",
    "- **`most_frequent_config`**: Most frequently selected config across outer folds\n",
    "\n",
    "- **`best_by_performance`**: Best config based on raw metric (e.g., R² or F1)\n",
    "\n",
    "- **`overall_metrics`**: Contains the mean and standard deviation of the evaluation metrics computed on the outer test sets across all outer folds. This provides a global estimate of model generalization performance.\n",
    "\n",
    "- **`configs_summary`**: A ranked summary of all unique configurations selected during outer folds, including their raw score, weighted score, selection frequency, and corresponding ranks.\n",
    "\n",
    "- **`fold_metrics`**: Lists the evaluation results for each outer fold. For every fold, it includes: the fold index, the best configuration selected via inner CV, and the model’s performance on the corresponding outer test set.\n",
    "\n",
    "- **`execution_time`**: Total time elapsed (seconds)\n",
    "\n",
    "- **`weighting_factors`**: Current weights used to compute the final score\n",
    "\n",
    "\n",
    "##### **Notes**:\n",
    "\n",
    "Supports both regression and classification tasks.\n",
    "\n",
    "Handles errors and logs metrics throughout execution.\n",
    "\n",
    "Designed for modular, extensible HPO pipelines with pluggable strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GRID SEARCH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search is an exhaustive search strategy that systematically evaluates all possible combinations of hyperparameters defined in a user-specified grid. Each hyperparameter is associated with a discrete set of candidate values, and the Cartesian product of these sets is computed to form the full configuration space.\n",
    "\n",
    "For example, given a grid like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"lr\": [0.01, 0.001],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "}\n",
    "```\n",
    "Grid Search will generate all 2 x 2 x 2 = 8 possible combinations, such as:\n",
    "\n",
    "```python\n",
    "{'lr': 0.01, 'batch_size': 32, 'optimizer': 'adam'}\n",
    "{'lr': 0.01, 'batch_size': 32, 'optimizer': 'sgd'}\n",
    "...\n",
    "{'lr': 0.001, 'batch_size': 64, 'optimizer': 'sgd'},\n",
    "```\n",
    "\n",
    "This approach guarantees that the best-performing configuration within the provided grid will be found (assuming proper evaluation), but its cost grows exponentially with the number of hyperparameters and candidate values.\n",
    "\n",
    "##### **Use this method when**:\n",
    "\n",
    "- The search space is relatively small\n",
    "\n",
    "- Full coverage is required\n",
    "\n",
    "- Parallel evaluation is feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **How to use Grid Search in AutoHyper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform hyperparameter optimization using Grid Search, simply call the **`hp_tuning`** method with **`hpo_method=\"grid_search\"`** and specify the number of outer and inner cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] Initialized HPO class\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] ################################################ HPO ################################################\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] Starting Nested Cross-Validation for HPO\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] Number of outer folds: 5, Number of inner folds: 3, Method: grid_search\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] Target task: regression\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] OUTER FOLD 1/5 STARTED\u001b[0m\n",
      "[2025-06-28 15:59:38] [\u001b[32mINFO\u001b[0m] Starting Grid Search...\u001b[0m\n",
      "[2025-06-28 15:59:46] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 40, 'min_samples_split': 30, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 15:59:46] [\u001b[32mINFO\u001b[0m] Outer fold 1 results - R2: 0.7270, MSE: 0.3578 for congiguration: {'max_depth': 40, 'min_samples_split': 30, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 15:59:46] [\u001b[32mINFO\u001b[0m] OUTER FOLD 1/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 15:59:46] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 15:59:46] [\u001b[32mINFO\u001b[0m] OUTER FOLD 2/5 STARTED\u001b[0m\n",
      "[2025-06-28 15:59:46] [\u001b[32mINFO\u001b[0m] Starting Grid Search...\u001b[0m\n",
      "[2025-06-28 15:59:59] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 15:59:59] [\u001b[32mINFO\u001b[0m] Outer fold 2 results - R2: 0.7324, MSE: 0.3656 for congiguration: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 15:59:59] [\u001b[32mINFO\u001b[0m] OUTER FOLD 2/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 15:59:59] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 15:59:59] [\u001b[32mINFO\u001b[0m] OUTER FOLD 3/5 STARTED\u001b[0m\n",
      "[2025-06-28 15:59:59] [\u001b[32mINFO\u001b[0m] Starting Grid Search...\u001b[0m\n",
      "[2025-06-28 16:00:13] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 16:00:13] [\u001b[32mINFO\u001b[0m] Outer fold 3 results - R2: 0.7113, MSE: 0.3756 for congiguration: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 16:00:13] [\u001b[32mINFO\u001b[0m] OUTER FOLD 3/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:13] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:13] [\u001b[32mINFO\u001b[0m] OUTER FOLD 4/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:00:13] [\u001b[32mINFO\u001b[0m] Starting Grid Search...\u001b[0m\n",
      "[2025-06-28 16:00:24] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 16:00:25] [\u001b[32mINFO\u001b[0m] Outer fold 4 results - R2: 0.7384, MSE: 0.3488 for congiguration: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 16:00:25] [\u001b[32mINFO\u001b[0m] OUTER FOLD 4/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:25] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:25] [\u001b[32mINFO\u001b[0m] OUTER FOLD 5/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:00:25] [\u001b[32mINFO\u001b[0m] Starting Grid Search...\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 40, 'min_samples_split': 30, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Outer fold 5 results - R2: 0.7207, MSE: 0.3759 for congiguration: {'max_depth': 40, 'min_samples_split': 30, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0}\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] OUTER FOLD 5/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Outer CV completed, Results:\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Best weighted configuration: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0} with weight score 0.8091\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Most frequent configuration: {'max_depth': 40, 'min_samples_leaf': 15, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0} (appeared 3 times)\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Best by performance: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0} with performance score 0.7274\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Total execution time: 57.41 seconds\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] ================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Hyperparameter optimization complete :)\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] ================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_hpo_grid = HPO(\n",
    "    model=model,\n",
    "    data_features=data_features,\n",
    "    data_target=data_target,\n",
    "    hp_values=hp_values,\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "grid_search = test_hpo_grid.hp_tuning(hpo_method=\"grid_search\", outer_k=5, inner_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RANDOM SEARCH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search is a stochastic search strategy that samples a fixed number of random combinations from the hyperparameter grid.\n",
    "Unlike Grid Search, it does not evaluate all possible configurations, but instead selects a random subset, making it more scalable for high-dimensional search spaces.\n",
    "\n",
    "For example, given a grid like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"lr\": [0.01, 0.001],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "}\n",
    "```\n",
    "You can specify the number of configurations to evaluate with **`n_trials`**. For instance, with **`n_trials=4`**, Random Search might sample:\n",
    "\n",
    "```python\n",
    "{'lr': 0.001, 'batch_size': 64, 'optimizer': 'adam'}\n",
    "{'lr': 0.01, 'batch_size': 32, 'optimizer': 'sgd'}\n",
    "{'lr': 0.01, 'batch_size': 64, 'optimizer': 'adam'}\n",
    "{'lr': 0.001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
    "```\n",
    "\n",
    "These are randomly selected from the full Cartesian product of the parameter space.\n",
    "\n",
    "This approach does not guarantee finding the globally best configuration, but it is often more efficient and surprisingly effective in practice, especially when only a few hyperparameters dominate model performance.\n",
    "\n",
    "##### **Use this method when**:\n",
    "\n",
    "- The search space is large or high-dimensional\n",
    "\n",
    "- You want faster optimization with fewer evaluations\n",
    "\n",
    "- Full coverage is unnecessary or too costly\n",
    "\n",
    "- You can afford randomness in exchange for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **How to Use Random Search in AutoHyper**\n",
    "\n",
    "To perform hyperparameter optimization using Random Search, call the **`hp_tuning`** method with **`hpo_method=\"random_search\"`** and specify the number of configurations to sample via the **`n_trials`** parameter as well as the number of outer and inner cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Initialized HPO class\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] ################################################ HPO ################################################\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Starting Nested Cross-Validation for HPO\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Number of outer folds: 5, Number of inner folds: 3, Method: random_search\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Target task: regression\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] OUTER FOLD 1/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:00:36] [\u001b[32mINFO\u001b[0m] Starting Random Search...\u001b[0m\n",
      "[2025-06-28 16:00:40] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 26, 'min_samples_split': 21, 'min_samples_leaf': 6, 'min_weight_fraction_leaf': 0.013308544647661624}\u001b[0m\n",
      "[2025-06-28 16:00:40] [\u001b[32mINFO\u001b[0m] Outer fold 1 results - R2: 0.6328, MSE: 0.4812 for congiguration: {'max_depth': 26, 'min_samples_split': 21, 'min_samples_leaf': 6, 'min_weight_fraction_leaf': 0.013308544647661624}\u001b[0m\n",
      "[2025-06-28 16:00:40] [\u001b[32mINFO\u001b[0m] OUTER FOLD 1/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:40] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:40] [\u001b[32mINFO\u001b[0m] OUTER FOLD 2/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:00:40] [\u001b[32mINFO\u001b[0m] Starting Random Search...\u001b[0m\n",
      "[2025-06-28 16:00:45] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 40, 'min_samples_split': 25, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0062483006776456575}\u001b[0m\n",
      "[2025-06-28 16:00:45] [\u001b[32mINFO\u001b[0m] Outer fold 2 results - R2: 0.6829, MSE: 0.4331 for congiguration: {'max_depth': 40, 'min_samples_split': 25, 'min_samples_leaf': 15, 'min_weight_fraction_leaf': 0.0062483006776456575}\u001b[0m\n",
      "[2025-06-28 16:00:45] [\u001b[32mINFO\u001b[0m] OUTER FOLD 2/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:45] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:45] [\u001b[32mINFO\u001b[0m] OUTER FOLD 3/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:00:45] [\u001b[32mINFO\u001b[0m] Starting Random Search...\u001b[0m\n",
      "[2025-06-28 16:00:48] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 24, 'min_samples_split': 22, 'min_samples_leaf': 4, 'min_weight_fraction_leaf': 0.003963305660432947}\u001b[0m\n",
      "[2025-06-28 16:00:48] [\u001b[32mINFO\u001b[0m] Outer fold 3 results - R2: 0.6968, MSE: 0.3945 for congiguration: {'max_depth': 24, 'min_samples_split': 22, 'min_samples_leaf': 4, 'min_weight_fraction_leaf': 0.003963305660432947}\u001b[0m\n",
      "[2025-06-28 16:00:48] [\u001b[32mINFO\u001b[0m] OUTER FOLD 3/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:48] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:48] [\u001b[32mINFO\u001b[0m] OUTER FOLD 4/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:00:48] [\u001b[32mINFO\u001b[0m] Starting Random Search...\u001b[0m\n",
      "[2025-06-28 16:00:53] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 12, 'min_weight_fraction_leaf': 0.01003234894725893}\u001b[0m\n",
      "[2025-06-28 16:00:53] [\u001b[32mINFO\u001b[0m] Outer fold 4 results - R2: 0.6904, MSE: 0.4127 for congiguration: {'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 12, 'min_weight_fraction_leaf': 0.01003234894725893}\u001b[0m\n",
      "[2025-06-28 16:00:53] [\u001b[32mINFO\u001b[0m] OUTER FOLD 4/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:53] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:53] [\u001b[32mINFO\u001b[0m] OUTER FOLD 5/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:00:53] [\u001b[32mINFO\u001b[0m] Starting Random Search...\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Best config found from the Inner CV loop: {'max_depth': 35, 'min_samples_split': 29, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.008682424243927534}\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Outer fold 5 results - R2: 0.6498, MSE: 0.4713 for congiguration: {'max_depth': 35, 'min_samples_split': 29, 'min_samples_leaf': 5, 'min_weight_fraction_leaf': 0.008682424243927534}\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] OUTER FOLD 5/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Outer CV completed, Results:\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Best weighted configuration: {'max_depth': 24, 'min_samples_split': 22, 'min_samples_leaf': 4, 'min_weight_fraction_leaf': 0.003963305660432947} with weight score 0.7878\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Most frequent configuration: {'max_depth': 26, 'min_samples_leaf': 6, 'min_samples_split': 21, 'min_weight_fraction_leaf': 0.013308544647661624} (appeared 1 times)\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Best by performance: {'max_depth': 24, 'min_samples_split': 22, 'min_samples_leaf': 4, 'min_weight_fraction_leaf': 0.003963305660432947} with performance score 0.6968\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Total execution time: 19.36 seconds\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] ================================================================================\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] Hyperparameter optimization complete :)\u001b[0m\n",
      "[2025-06-28 16:00:56] [\u001b[32mINFO\u001b[0m] ================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_hpo_random = HPO(\n",
    "    model=model,\n",
    "    data_features=data_features,\n",
    "    data_target=data_target,\n",
    "    hp_values=hp_values,\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "random_search = test_hpo_random.hp_tuning(\n",
    "    hpo_method=\"random_search\", outer_k=5, inner_k=3, n_trials=27\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **EVOLUTIONARY ALGORITHM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Evolutionary Algorithm is a population-based, bio-inspired optimization strategy that evolves hyperparameter configurations over multiple generations.\n",
    "Each generation refines the search by selecting the best-performing configurations (parents), applying mutations to generate offspring, and retaining the fittest solutions.\n",
    "\n",
    "This approach is especially useful for exploring large or complex search spaces, where exhaustive methods are impractical and random search may be inefficient.\n",
    "\n",
    "##### **Workflow**\n",
    "\n",
    "1. **Initialization**:\n",
    "\n",
    "A population of candidate configurations is generated either from the Cartesian product of the hyperparameter space or by sampling additional random configurations (if **`n_new_configs`** is specified). Each configuration is validated for compatibility with the model.\n",
    "\n",
    "2. **Evaluation**:\n",
    "\n",
    "Each configuration is evaluated using K-fold cross-validation, and assigned a fitness score (mean accuracy for classification, or mean R² for regression).\n",
    "\n",
    "3. **Parent Selection**:\n",
    "\n",
    "A fraction of the evaluated configurations is selected as parents based on the specified mechanism:\n",
    "\n",
    "**`\"neutral_selection\"`**: random selection (no fitness pressure) (very computationally cheap)\n",
    "\n",
    "**`\"fitness_proportional_selection\"`**: selection with a probability proportional to the fitness (higher fitness = higher chance of selection) (very computationally expensive)\n",
    "\n",
    "**`\"tournament_selection\"`**: select the best from randomly drawn groups (moderately computationally expensive)\n",
    "\n",
    "4. **Mutation**:\n",
    "\n",
    "Each parent undergoes mutation:\n",
    "\n",
    "- Numerical parameters (int/float) are incremented upward or downward (with a mechanism that checks the validy of the values for safety).\n",
    "\n",
    "- Categorical parameters are replaced with a different value.\n",
    "\n",
    "\n",
    "5. **Survival (μ + λ)**:\n",
    "\n",
    "μ = number of individuals in the current generation (parents)\n",
    "\n",
    "λ = number of offspring generated in the current generation\n",
    "\n",
    "The best μ configurations among both parents and offspring (μ + λ) are selected for the next generation.\n",
    "\n",
    "\n",
    "6. **Tracking & Convergence**:\n",
    "\n",
    "For each generation, statistics are collected:\n",
    "\n",
    "Best and mean fitness, Population diversity, Parameter evolution over time\n",
    "\n",
    "The process continues for **`max_generations`**, after which the best configuration is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **How to Use Evolutionary Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Hyperparameter optimization with evolutionary algorithm, call the **`hp_tuning`** method and activate it via **`hp_tuning(hpo_method=\"evolutionary_algorithm\")`** and specify the evolutionary options below:\n",
    "\n",
    "##### **Evolution Specific Parameters**\n",
    "\n",
    "| Parameter                     | Type                  | Required | Description                                                                                                                                                                                  |\n",
    "| ----------------------------- | --------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `parents_selection_mechanism` | `str`                 | **Yes**  | Parent selection method:<br>• `\"neutral_selection\"` – random<br>• `\"fitness_proportional_selection\"` – probability proportional to fitness<br>• `\"tournament_selection\"` – best of random mini-tournaments |\n",
    "| `parents_selection_ratio`     | `float`               | **Yes**  | Fraction of the current population chosen as parents (0 < ratio ≤ 1).                                                                                                                        |\n",
    "| `max_generations`             | `int`                 | **Yes**  | Number of evolutionary iterations to run.                                                                                                                                                    |\n",
    "| `n_new_configs`               | `Optional[int]`       | No       | Extra random configurations added to the initial population (default `None`).                                                                                                                |                                                                                                       |\n",
    "\n",
    "\n",
    "##### **When to choose EA**\n",
    "\n",
    "- Very large or mixed-type hyperparameter spaces\n",
    "\n",
    "- Desire for self-adapting exploration vs. exploitation\n",
    "\n",
    "- Need for convergence diagnostics (fitness trends, diversity)\n",
    "\n",
    "- Exhaustive grid is infeasible and random search under-explores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] Initialized HPO class\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] ################################################ HPO ################################################\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] Starting Nested Cross-Validation for HPO\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] Number of outer folds: 5, Number of inner folds: 3, Method: evolutionary_algorithm\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] Target task: regression\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] OUTER FOLD 1/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:03:05] [\u001b[32mINFO\u001b[0m] Initial population size: 81\u001b[0m\n",
      "[2025-06-28 16:06:30] [\u001b[32mINFO\u001b[0m] Outer fold 1 results - R2: 0.6991, MSE: 0.3943 for congiguration: {'max_depth': 28, 'min_samples_split': 8, 'min_samples_leaf': 13, 'min_weight_fraction_leaf': 0.0031}\u001b[0m\n",
      "[2025-06-28 16:06:30] [\u001b[32mINFO\u001b[0m] OUTER FOLD 1/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:06:30] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:06:30] [\u001b[32mINFO\u001b[0m] OUTER FOLD 2/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:06:30] [\u001b[32mINFO\u001b[0m] Initial population size: 81\u001b[0m\n",
      "[2025-06-28 16:08:58] [\u001b[32mINFO\u001b[0m] Outer fold 2 results - R2: 0.7017, MSE: 0.4075 for congiguration: {'max_depth': 14, 'min_samples_split': 20, 'min_samples_leaf': 13, 'min_weight_fraction_leaf': 0.0041}\u001b[0m\n",
      "[2025-06-28 16:08:58] [\u001b[32mINFO\u001b[0m] OUTER FOLD 2/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:08:58] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:08:58] [\u001b[32mINFO\u001b[0m] OUTER FOLD 3/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:08:58] [\u001b[32mINFO\u001b[0m] Initial population size: 81\u001b[0m\n",
      "[2025-06-28 16:11:56] [\u001b[32mINFO\u001b[0m] Outer fold 3 results - R2: 0.6963, MSE: 0.3951 for congiguration: {'max_depth': 16, 'min_samples_split': 29, 'min_samples_leaf': 4, 'min_weight_fraction_leaf': 0.0044}\u001b[0m\n",
      "[2025-06-28 16:11:56] [\u001b[32mINFO\u001b[0m] OUTER FOLD 3/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:11:56] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:11:56] [\u001b[32mINFO\u001b[0m] OUTER FOLD 4/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:11:56] [\u001b[32mINFO\u001b[0m] Initial population size: 81\u001b[0m\n",
      "[2025-06-28 16:14:03] [\u001b[32mINFO\u001b[0m] Outer fold 4 results - R2: 0.6811, MSE: 0.4251 for congiguration: {'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.0097}\u001b[0m\n",
      "[2025-06-28 16:14:03] [\u001b[32mINFO\u001b[0m] OUTER FOLD 4/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:14:03] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:14:03] [\u001b[32mINFO\u001b[0m] OUTER FOLD 5/5 STARTED\u001b[0m\n",
      "[2025-06-28 16:14:03] [\u001b[32mINFO\u001b[0m] Initial population size: 81\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] Outer fold 5 results - R2: 0.6071, MSE: 0.5289 for congiguration: {'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 13, 'min_weight_fraction_leaf': 0.0227}\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] OUTER FOLD 5/5 COMPLETED\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] ================================================================================================\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] Outer CV completed, Results:\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] Best weighted configuration: {'max_depth': 14, 'min_samples_split': 20, 'min_samples_leaf': 13, 'min_weight_fraction_leaf': 0.0041} with weight score 0.7912\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] Most frequent configuration: {'max_depth': 28, 'min_samples_leaf': 13, 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.0031} (appeared 1 times)\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] Best by performance: {'max_depth': 14, 'min_samples_split': 20, 'min_samples_leaf': 13, 'min_weight_fraction_leaf': 0.0041} with performance score 0.7017\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] Total execution time: 793.92 seconds\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] ================================================================================\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] Hyperparameter optimization complete :)\u001b[0m\n",
      "[2025-06-28 16:16:20] [\u001b[32mINFO\u001b[0m] ================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_hpo_ea = HPO(\n",
    "    model=model,\n",
    "    data_features=data_features,\n",
    "    data_target=data_target,\n",
    "    hp_values=hp_values,\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "evolutionary_algo = test_hpo_ea.hp_tuning(\n",
    "    hpo_method=\"evolutionary_algorithm\",\n",
    "    outer_k=5,\n",
    "    inner_k=3,\n",
    "    parents_selection_mechanism=\"tournament_selection\",\n",
    "    parents_selection_ratio=0.5,\n",
    "    n_new_configs=10,\n",
    "    max_generations=25,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
