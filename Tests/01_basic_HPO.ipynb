{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TO DO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[X] i changed the output structure of the tuning parameter i need to understand it more clearly, in successive phase i can reason more on the output structure to simplify it and enache visualization\n",
    "\n",
    "[X] create the random search method\n",
    "\n",
    "[X] sistema output hpo_tuning \n",
    "\n",
    "[X]create the logger mechanism to show the steps of the classifier / model so to have a nice visualization of the model behaviour (figo come in progetto avatar il fatto di mostrare una tabella markdown nel logger con i risultati invece di printare e quindi imparare il meccanismo dei logger fatti bene) \n",
    "in modo da non dare sempre il dizionario per il lato backend ma dare il dizionario solo se viene fatto print\n",
    "\n",
    "[X] logger implementato correttaemente, sistema da un punto di vista visuale la struttura dei logs\n",
    "\n",
    "[?] create the data visualization method for random search and web search and in general for the structured output (ho introdotto le visualization e la loro struttura ma non credo che vadano bene, in più non so nemmeno se sono visualizzazioni corrette, sarebbe meglio richiedere e farne per esempio solo 4 le più importanti, però il meccanismo l'ho implementato) -> \n",
    "\n",
    "[X]create the EA method\n",
    "\n",
    "[ ] creazione pacchetto e read me per pubblicazione\n",
    "\n",
    "[ ] creazione notebook di test completo e fatto bene con descrizione pacchetto come usarlo e ogni singolo metodo con base teorica\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "[ ]creazione primo noteboook per mostrare funzionamento del pacchetto su grid search e random search\n",
    "\n",
    "[ ] create the bayesian optmization method\n",
    "\n",
    "[ ] creazione secondo noteboook per mostrare funzionamento del pacchetto su EA in dettaglio\n",
    "\n",
    "[ ] pubblicazione pacchetto con spiegazione generale\n",
    "\n",
    "[ ] pubblicazione primo notebook di test random e grid search \n",
    "\n",
    "[ ] pubblicazione secondo notebook funzionamento EA \n",
    "\n",
    "[ ] creazione terzo notebook per mostrare funzionamento del pacchetto su Bayesian Opt\n",
    "\n",
    "[ ] pubblicazione terzo notebook Bayesian Opt\n",
    "\n",
    "[ ] creare metodo per hyperparameter importance, quindi implementare logiche per selezionare, valutare l'importanza dei singoli hyperparameter\n",
    "\n",
    "[ ] creare notebook per mostrare le strategie di HP importance\n",
    "\n",
    "[ ] pubblicare notebook hyperparameter importance\n",
    "\n",
    "[ ] creare automl pipeline method\n",
    "\n",
    "[ ] creare notebook per presentare automl pipeline\n",
    "\n",
    "FINE \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Basic HPO - Grid Search and Random Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the aim of this notebook is to make an unbiased comparison between two basic hyper parameter optimization technique:\n",
    "In order to explain and undestand the main differences and strenghts.\n",
    "\n",
    "- grid search\n",
    "\n",
    "- random search \n",
    "\n",
    "Breve spiegazione delle due tecniche (con pro/contro teorici).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from src.autohyper import HPO\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TESTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing Dataset and convert it into a pandas DataFrame\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "X = pd.DataFrame(\n",
    "    X,\n",
    "    columns=[\n",
    "        \"MedInc\",\n",
    "        \"HouseAge\",\n",
    "        \"AveRooms\",\n",
    "        \"AveBedrms\",\n",
    "        \"Population\",\n",
    "        \"AveOccup\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "    ],\n",
    ")\n",
    "y = pd.Series(y, name=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "data_features = X\n",
    "data_target = y\n",
    "hp_values = {\n",
    "    \"max_depth\": [1, 3, 5],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"n_estimators\": [10, 30, 50],\n",
    "}\n",
    "task = \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hpo = HPO(\n",
    "    model=model,\n",
    "    data_features=data_features,\n",
    "    data_target=data_target,\n",
    "    hp_values=hp_values,\n",
    "    task=task,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **01. GRID SEARCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = test_hpo.hp_tuning(hpo_method=\"grid_search\", outer_k=5, inner_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **02. Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = test_hpo.hp_tuning(\n",
    "    hpo_method=\"random_search\", outer_k=5, inner_k=3, n_trials=35\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **03. Evolutionary Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolutionary_algo = test_hpo.hp_tuning(\n",
    "    hpo_method=\"evolutionary_algorithm\",\n",
    "    outer_k=5,\n",
    "    inner_k=3,\n",
    "    parents_selection_mechanism=\"fitness_proportional_selection\",\n",
    "    parents_selection_ratio=0.5,\n",
    "    n_new_configs=10,\n",
    "    max_generations=20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
